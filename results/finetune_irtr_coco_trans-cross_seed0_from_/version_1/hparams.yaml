config:
  amp_level: O1
  apex: false
  batch_size: 512
  betas:
  - 0.9
  - 0.98
  data_root: /root/autodl-tmp/datas/coco2014_karpathy_prepared
  datasets:
  - coco
  decay_power: 1
  draw_false_image: 1
  draw_false_text: 0
  drop_rate: 0.1
  end_lr: 0
  eps: 1.0e-08
  exp_name: finetune_irtr_coco
  fast_dev_run: false
  get_recall_metric: true
  hidden_size: 768
  image_only: false
  image_size: 224
  input_image_embed_size: 768
  input_text_embed_size: 768
  learning_rate: 2.0e-05
  load_path: ''
  loss_name:
    contras: 0
    irtr: 1
    itm: 1
    mlm: 0
    mpp: 0
    nlvr2: 0
    snli: 0
    vcr: 0
    vcr_qar: 0
    vqa: 0
  lr_mult_cross_modal: 5
  lr_mult_head: 5
  max_epoch: 10
  max_steps: -1
  max_text_len: 40
  mlm_prob: 0.15
  mlp_ratio: 1
  momentum: 0.0
  num_gpus:
  - 0
  - 1
  num_heads: 12
  num_layers: 6
  num_nodes: 1
  num_sanity_val_steps: 0
  num_top_layer: 2
  num_workers: 8
  optim_type: adamw
  output_dir: /root/autodl-tmp/result
  patch_size: 32
  per_gpu_batchsize: 32
  pin_memory: true
  precision: 32
  recall_topk: 256
  representation_fn: trans-cross
  seed: 0
  send_name: project
  shuffle: true
  test_dataset_len: -1
  test_only: false
  tokenizer: /root/autodl-tmp/pretrained_models/en-roberta-base
  tokenizer_name: roberta-base
  train_dataset_len: -1
  train_transform_keys:
  - clip_randaug
  val_check_interval: 0.5
  val_dataset_len: -1
  val_transform_keys:
  - clip
  vit: /root/autodl-tmp/pretrained_models/ViT/ViT-B-32.pt
  vit_name: ViT-B-32
  vocab_size: 50265
  vqav2_label_size: 3129
  warmup_steps: 0.1
  weight_decay: 0.01
  whole_word_masking: false

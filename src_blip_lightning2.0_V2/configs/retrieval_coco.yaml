# 常规设置
datasets: ['coco']
data_root: "/home/tzj/datas/mscoco/coco2014_karpathy_prepared"
output_dir: "../outputs"
log_dir: "../logs_blip"
# set pretrained as a file path or an url
pretrained: '/home/tzj/pretrained_models/blip/model_base.pth'

seed: 42
train_dataset_len: -1
val_dataset_len: -1
test_dataset_len: -1
shuffle: True # 训练集是否打乱
task_name: {'irtr': 1}
get_recall_metric: True
top_k: 64
#queue_size: 16000
queue_size: 57600
momentum: 0.995
alpha: 0.4
negative_all_rank: True

# ----------------------  Lightning Trainer Setting  ------------------------------
#load_path: ""  # 模型权重地址
num_sanity_val_steps: 0 # 在开始前取 n 个val batches
fast_dev_run: False # 快速检验，取 n 个train, val, test batches
val_check_interval: 1.0 # 验证间隔（浮点数为每X个epoch验证一次，整数为每X个step验证一次）
accelerator: 'gpu'
devices: [3,4]
batch_size: 128  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.
per_gpu_batchsize: 32  # you should define this manually with per_gpu_batch_size=#
num_nodes: 1
pin_memory: True
num_workers: 8
precision: 32
max_grad_norm: 1.
max_epoch: 6
max_steps: -1
warmup_steps: 0.15

# ----------------------  Image Setting  ----------------------
vit_name: 'ViT-B-32'
vit: 'base'
#vit: '/home/tzj/pretrained_models/ViT/ViT-B-32.pt'  # vit模型权重
#vit_name: 'ViT-B-16'
#vit: '/home/tzj/pretrained_models/ViT/ViT-B-16.pt'  # vit模型权重
image_size: 224 # 调整后的图片像素
patch_size: 16 # 送入vit的图片块的像素
train_transform_keys: ["clip"] # 预处理图片的模型（将图片处理为image_size * image_size）
val_transform_keys: ["clip"]
input_image_embed_size: 256
vit_grad_ckpt: True
vit_ckpt_layer: 4


# ----------------------  Text Setting  ----------------------
tokenizer_name: 'roberta-base'
tokenizer: '/home/tzj/pretrained_models/en-roberta-base'
vocab_size: 50265
input_text_embed_size: 256
max_text_len: 40
whole_word_masking: False # note that whole_word_masking does not work for RoBERTa
mlm_prob: 0.15 # mlm遮罩比例

# ----------------------  Transformer Setting  ----------------------
#num_top_layer: 6  # 融合模块的层数
#input_image_embed_size: 768
#input_text_embed_size: 768
#hidden_size: 768
#num_heads: 12 # 注意力的head数量
#num_layers: 6 #
## mlp_ratio = 1 # 中间层的维度：hidden_size * mlp_ratio
#mlp_ratio: 4 # 中间层的维度：hidden_size * mlp_ratio
#drop_rate: 0.1 # dropout

# ----------------------  Optimizer Setting  ----------------------
optimizer:
  optim_type: 'adamw'
  init_lr: 1e-5
  #learning_rate: 1e-5,
  min_lr: 0
  eps: 1e-8
  betas: (0.9, 0.98)
  weight_decay: 0.05
#  decay_power: 1
#  max_epoch: 6,
#  max_steps: -1,
  lr_mult_head: 5 # multiply lr for downstream heads
  lr_mult_cross_modal: 5  # multiply lr for the cross-modal module

scheduler: 'linear'
#scheduler: {
#  sched: cosine,
#  lr: 1e-5,
#  min_lr: 1e-6,
#  decay_rate: 1,
#  warmup_lr: 1e-5,
#  warmup_epochs: 1,
#  cooldown_epochs: 0,
#}
